---
title: Redis学习总结
date: 2021-04-26 20:27:13
permalink: /pages/b9a12d/
categories:
  - 数据库
  - Redis
tags:
  - 
---


# Redis学习总结

## 1.简单介绍下Redis（Redis的含义和作用）

**Redis**是一个使用c语音开发的数据库，不过与传统的数据库不同的是**Redis的数据是存储在内存中的**，也就是**Redis是内存数据库**，所以读写速度特别快，因此Redis被广泛应用于**缓存**方向。

Redis除了做缓存外，也经常被用来做分布式锁，甚至是消息队列。

Redis默认是有16个数据库。

Redis提供了多种数据类型来支持不同的业务场景。Redis还支持事务，持久化，Lua脚本，多种集群方案。

## 2.Redis一些简单命令的使用

### 2.1 Redis性能测试

**语法：**

```java
redis-benchmark [option] [option value]
```

**参数：**

![image-20210327094708401](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327094716.png)

**实例：**

、测试100个并发每秒10w个请求

![image-20210327095549330](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327095550.png)

### 2.2 其他命令

**选择不同的数据库：*select [num]***

```java
> select 2
OK
```

**查看当前数据库的所有key：*keys \****

![image-20210327100146974](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327100148.png)

**设置key：*set [key] [value]* 和获取key：*get [key]***

![image-20210327100438475](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327100439.png)

**查看当前数据库的key的数量：*DBSIZE***

```java
> DBSIZE
16
```

**清空当前的数据库：*FLUSHDB***

![image-20210327100902117](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327100903.png)

**清空所有数据库：*FLUSHALL***

![image-20210327101004996](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327101005.png)

**判断某个key是否存在：*EXISTS [KEY]***

![image-20210327125600154](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327125601.png)

**移动某个key-value到其他数据库：*move [nums]***

![image-20210327130437929](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327130439.png)

**设置过期时间：*EXPIRE [keys] [seconds]***

![image-20210327130950586](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327130952.png)

**查看key的一个类型：*type [key]***

![image-20210327131448590](https://gitee.com/zhangrenfeng/images/raw/master/img/20210327131449.png)



## 3. Redis常用的五大数据类型



### 3.1 String类型

1. **介绍** ：string 数据结构是简单的 key-value 类型。虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 **简单动态字符串**（simple dynamic string，**SDS**）。相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外,Redis 的 SDS API 是安全的，不会造成缓冲区溢出。
2. 应用场景：一般用在需要计数的场景，比如用户的访问次数，文章点赞量阅读量等等。
3. 命令的使用：

**普通字符串的基本操作：**

```bash
127.0.0.1:6379> PING
PONG
127.0.0.1:6379> SET name hello # 设置key-value 类型的值
OK
127.0.0.1:6379> get name   ## 根据key获取对应的value
"hello"
127.0.0.1:6379> exists name  ## 判断key是否存在
(integer) 1
127.0.0.1:6379> APPEND name "!datealive"  ##追加字符串，如果当前的key不存在，就相当于set key
(integer) 15
127.0.0.1:6379> get name 
"hello!datealive"
127.0.0.1:6379> strlen name  ## 获取某个key的value长度
(integer) 15
127.0.0.1:6379> APPEND name !! 
(integer) 17
127.0.0.1:6379> get name
"hello!datealive!!"
127.0.0.1:6379> APPEND name1 123
(integer) 3
127.0.0.1:6379> get name
"hello!datealive!!"
127.0.0.1:6379> get name1
"123"
127.0.0.1:6379> del name  ##删除key
(integer) 1
127.0.0.1:6379> keys *  ## 查看所有的key
1) "name1"
127.0.0.1:6379> get name
(nil)
127.0.0.1:6379>
```

**计数器（字符串内容为整数时使用）：**

```bash
127.0.0.1:6379> set view 0  ## 设置文章浏览量为0
OK
127.0.0.1:6379> get view
"0"
127.0.0.1:6379> type view
string
127.0.0.1:6379> incr view ##自增1   浏览量+1
(integer) 1
127.0.0.1:6379> get view
"1"
127.0.0.1:6379> decr view ##自减1   浏览量-1
(integer) 0
127.0.0.1:6379> get view
"0"
127.0.0.1:6379> INCRBY view 10 ##可指定步长，指定增量 浏览量+10
(integer) 10
127.0.0.1:6379> get view
"10"
127.0.0.1:6379> DECRBY view 10 ##可指定步长，指定减量 浏览量-10
(integer) 0
127.0.0.1:6379> get view
"0"
```

**字符串的截取和替换：**

```bash
127.0.0.1:6379> set hi "hello,datealive"
OK
127.0.0.1:6379> get hi
"hello,datealive"
127.0.0.1:6379> GETRANGE hi 0 3 ##截取字符串  [0,3]
"hell"
127.0.0.1:6379> GETRANGE hi 0 -1 ##截取字符串
"hello,datealive"
127.0.0.1:6379> set hello "abbdefg"
OK
127.0.0.1:6379> get hello
"abbdefg"
127.0.0.1:6379> SETRANGE hello 2 c ##替换指定位置开始的字符串
(integer) 7
127.0.0.1:6379> get hello
"abcdefg"
127.0.0.1:6379> SETRANGE hello 2 cccccc
(integer) 8
127.0.0.1:6379> get hello
"abcccccc" 
```

**设置过期时间：**

```bash
127.0.0.1:6379> setex key1 30 hello #设置key1的值为hello，30秒后过期
OK
127.0.0.1:6379> ttl key1
(integer) 25
127.0.0.1:6379> get key1
"hello"
127.0.0.1:6379> setnx mykey "redis" ##如果mykey不存在，创建mykey
(integer) 1
127.0.0.1:6379> keys *
1) "mykey"
127.0.0.1:6379> setnx mykey "MongDB" #如果mykey存在，创建失败
(integer) 0
127.0.0.1:6379> keys *
1) "mykey"
127.0.0.1:6379> get mykey
"redis"
127.0.0.1:6379> EXPIRE mykey 60 ## 设置过期时间60秒
(integer) 1
127.0.0.1:6379> ttl mykey
(integer) 56
```

**批量操作：**

```bash
127.0.0.1:6379> mset k1 v1 k2 v2 k3 v3 #批量设置 key-value类型的值
OK
127.0.0.1:6379> mget k1 k2 k3  # 批量获取多个key对应的value值
1) "v1"
2) "v2"
3) "v3"
127.0.0.1:6379> msetnx k1 v1 k4 v4 #msetnx 是一个原子型操作 要么一起成功 要么一起失败
(integer) 0
127.0.0.1:6379> get k4
(nil)
```

**key的一个巧妙设计 user:{id}:{filed}**

```bash
127.0.0.1:6379> set user:1 {name:zhangsan,age:3} # 设置一个user:1 对象 值为json字符来保存一个对象
OK
127.0.0.1:6379> get user:1
"{name:zhangsan,age:3}"
127.0.0.1:6379> mset user:1:name zhangsan user:1:age 2
OK
127.0.0.1:6379> mget user:1:name user:1:age
1) "zhangsan"
2) "2"
```

**getset操作  先get然后set**

```bash
127.0.0.1:6379> getset db redis #如果不存在值，则返回nil
(nil)
127.0.0.1:6379> get db
"redis"
127.0.0.1:6379> getset db mongodb  #如果存在值，则返回旧的值  更新新的值
"redis"
127.0.0.1:6379> get db
"mongodb"
```

### 3.2 List 列表类型

1. **介绍** ：**list** 即是 **链表**。链表是一种非常常见的数据结构，特点是易于数据元素的插入和删除并且且可以灵活调整链表长度，但是链表的随机访问困难。许多高级编程语言都内置了链表的实现比如 Java 中的 **LinkedList**，但是 C 语言并没有实现链表，所以 Redis 实现了自己的链表数据结构。Redis 的 list 的实现为一个 **双向链表**，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。

2. **应用场景:** 发布与订阅或者说消息队列、慢查询。

3. **命令的使用：**

   **`lpush`，`rpush`，`lpop`，`rpop`，`lrange`基础命令用法：**

```bash
127.0.0.1:6379> lpush mylist one  #向列表的头部（左边）添加元素 如果key不存在，就会创建一个新的list
(integer) 1
127.0.0.1:6379> lpush mylist ttwo
(integer) 2
127.0.0.1:6379> lpush mylist three
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1 # 查看对应下标的list列表 start，end
1) "three"
2) "ttwo"
3) "one"
127.0.0.1:6379> rpush mylist zero
(integer) 4
127.0.0.1:6379> lrange mylist 0 -1
1) "three"
2) "ttwo"
3) "one"
4) "zero"
127.0.0.1:6379> lpop mylist #取出最左边元素
"three"
127.0.0.1:6379> lrange mylist 0 -1
1) "ttwo"
2) "one"
3) "zero"
127.0.0.1:6379> rpop mylist #取出最右边的元素
"zero"
127.0.0.1:6379> lrange mylist 0 -1
1) "ttwo"
2) "one"
```

`lindex,llen`的用法

```bash
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "two"
3) "three"
127.0.0.1:6379> lindex mylist 1 # 根据下标获取list中某个值
"two"
127.0.0.1:6379> lindex mylist 0
"one"
127.0.0.1:6379> llen mylist # 获取list的长度
(integer) 3
```

**lrem的使用：**

```bash
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "one"
3) "one"
4) "two"
5) "two"
6) "three"
127.0.0.1:6379> lrem mylist  1 one  # 移除list中指定个数的value 精确匹配
(integer) 1
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "one"
3) "two"
4) "two"
5) "three"
127.0.0.1:6379> lrem mylist  2 one
(integer) 2
127.0.0.1:6379> lrange mylist 0 -1
1) "two"
2) "two"
3) "three"
127.0.0.1:6379> lrem mylist  2 one
(integer) 0
127.0.0.1:6379> lrange mylist 0 -1
1) "two"
2) "two"
3) "three"
127.0.0.1:6379> lrem mylist 3 two
(integer) 2
127.0.0.1:6379> lrange mylist 0 -1
1) "three"
127.0.0.1:6379> lrem mylist 1 three
(integer) 1
127.0.0.1:6379> lrange mylist 0 -1
(empty list or set)
```

**ltrim的使用：**

```bash
127.0.0.1:6379> lrange mylist 0 -1
1) "hello"
2) "hello1"
3) "hello2"
4) "hello3"
127.0.0.1:6379> ltrim mylist 1 2 # 通过下标截取指定的长度  原先的list已经发生改变，只剩下了截取后的元素
OK
127.0.0.1:6379> lrange mylist 0 -1
1) "hello1"
2) "hello2"
```

**rpoplpush的使用：** 

```bash
127.0.0.1:6379> rpush mylist one
(integer) 1
127.0.0.1:6379> rpush mylist two
(integer) 2
127.0.0.1:6379> rpush mylist three
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "two"
3) "three"
127.0.0.1:6379> rpoplpush mylist mycopylist #移除列表最后一个元素，将他移动到新的列表中
"three"
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "two"
127.0.0.1:6379> lrange mycopylist 0 -1
1) "three"
127.0.0.1:6379> rpoplpush mylist mycopylist
"two"
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
127.0.0.1:6379> lrange mycopylist 0 -1
1) "two"
2) "three"
```

**lset和linsert的使用：**

```bash
127.0.0.1:6379> EXISTS mylist #判断这个key是否存在
(integer) 0
127.0.0.1:6379> lset mylist 0 value # 如果key不存在  更新就会报错
(error) ERR no such key
127.0.0.1:6379> rpush mylist one two three
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1
1) "one"
2) "two"
3) "three"
127.0.0.1:6379>
127.0.0.1:6379> lset mylist 0 zero # 如果key存在，更新指定下标的元素
OK
127.0.0.1:6379> lrange mylist 0 -1
1) "zero"
2) "two"
3) "three"
127.0.0.1:6379> lset mylist 3 four #如果key存在，但下标超出范围，会报错
(error) ERR index out of range
127.0.0.1:6379> lrange mylist 0 -1
1) "zero"
2) "two"
3) "three"
127.0.0.1:6379> linsert mylist before two one #将某个具体的值插入到指定元素的前面或者后面
(integer) 4
127.0.0.1:6379> lrange mylist 0 -1
1) "zero"
2) "one"
3) "two"
4) "three"
127.0.0.1:6379> linsert mylist after three four
(integer) 5
127.0.0.1:6379> lrange mylist 0 -1
1) "zero"
2) "one"
3) "two"
4) "three"
5) "four"
```

**通过`rpush\lpop`实现队列：**

```bash
127.0.0.1:6379> rpush queue 1 #向list的头部添加元素
(integer) 1
127.0.0.1:6379> rpush queue 2
(integer) 2
127.0.0.1:6379> rpush queue 3
(integer) 3
127.0.0.1:6379> rpush queue 4
(integer) 4
127.0.0.1:6379> lrange queue 0 -1
1) "1"
2) "2"
3) "3"
4) "4"
127.0.0.1:6379> lpop queue
"1"
127.0.0.1:6379> lpop queue
"2"
127.0.0.1:6379> lpop queue
"3"
127.0.0.1:6379> lpop queue
"4"
127.0.0.1:6379> lpop queue
(nil)
```

**通过`rpush/rpop`实现栈：**

```bash
127.0.0.1:6379> rpush mylist 1 2 3 
(integer) 3
127.0.0.1:6379> lrange mylist 0 -1
1) "1"
2) "2"
3) "3"
127.0.0.1:6379> rpop mylist #将list的最右边元素取出
"3"
127.0.0.1:6379> rpop mylist
"2"
127.0.0.1:6379> rpop mylist
"1"
127.0.0.1:6379> EXISTS mylist
(integer) 0
```

![image-20210328154743675](https://gitee.com/zhangrenfeng/images/raw/master/img/20210328154751.png)

### 3.3 set类型

1. **介绍 ：** set 类似于 Java 中的 `HashSet` 。Redis 中的 set 类型是一种无序集合，集合中的元素没有先后顺序。当你需要存储一个列表数据，又不希望出现重复数据时，set 是一个很好的选择，并且 set 提供了判断某个成员是否在一个 set 集合内的重要接口，这个也是 list 所不能提供的。可以基于 set 轻易实现交集、并集、差集的操作。比如：你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。Redis 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。
2. **应用场景:** 需要存放的数据不能重复以及需要获取多个数据源交集和并集等场景。
3. **命令的使用：**

**基础命令使用：**

```bash
127.0.0.1:6379> sadd myset hello  #添加一个元素
(integer) 1
127.0.0.1:6379> sadd myset hello #不允许有重复的元素
(integer) 0
127.0.0.1:6379> sadd myset hello1
(integer) 1
127.0.0.1:6379> sadd myset hello2
(integer) 1
127.0.0.1:6379> smembers myset #查看set中所有的元素
1) "hello2"
2) "hello1"
3) "hello"
127.0.0.1:6379> sismember myset hello #判断某个元素是否存在set中
(integer) 1
127.0.0.1:6379> sismember myset hello3
(integer) 0
127.0.0.1:6379> scard myset #查看set的长度
(integer) 3
127.0.0.1:6379> srem myset hello1
(integer) 1
127.0.0.1:6379> smembers myset
1) "hello2"
2) "hello"


### 移除和随机获取set中的元素
127.0.0.1:6379> srem myset hello3 #移除set中指定的元素
(integer) 0
127.0.0.1:6379> srandmember myset #随机获取set中的元素
"hello"
127.0.0.1:6379> srandmember myset 2 #随机获取set中指定个数的元素
1) "hello"
2) "hello2"
127.0.0.1:6379> srandmember myset 3
1) "hello"
2) "hello2"
127.0.0.1:6379> smembers myset
1) "hello3"
2) "hello2"
3) "hello"

### 随机删除set中的元素和移动某个set到另外一个set操作

127.0.0.1:6379> spop myset #随机删除set中的元素
"hello2"
127.0.0.1:6379> spop myset
"hello"
127.0.0.1:6379> spop myset
"hello3"

127.0.0.1:6379> sadd myset 1 2 3
(integer) 3
127.0.0.1:6379> sadd myset2 4 5
(integer) 2
127.0.0.1:6379> smembers myset
1) "1"
2) "2"
3) "3"
127.0.0.1:6379> smembers myset2
1) "4"
2) "5"
127.0.0.1:6379> smove myset myset2 1 #将某个set中的元素移动到另外一个set集合中
(integer) 1
127.0.0.1:6379> smembers myset2
1) "1"
2) "4"

## 并集（sinter），交集（sunion），差集（sdiff）

127.0.0.1:6379> sadd xmfans 1 2 3 4 6
(integer) 5
127.0.0.1:6379> sadd zsfans 1 2 5 7 8
(integer) 5
127.0.0.1:6379> smembers xmfans
1) "1"
2) "2"
3) "3"
4) "4"
5) "6"
127.0.0.1:6379> smembers zsfans
1) "1"
2) "2"
3) "5"
4) "7"
5) "8"
127.0.0.1:6379> sdiff xmfans zsfans
1) "3"
2) "4"
3) "6"
127.0.0.1:6379> sinter xmfans zsfans
1) "1"
2) "2"
127.0.0.1:6379> sunion xmfans zsfans
1) "1"
2) "2"
3) "3"
4) "4"
5) "5"
6) "6"
7) "7"
8) "8"
```

### Hash类型

1. **介绍** ：hash 类似于 JDK1.8 前的 HashMap，内部实现也差不多(数组 + 链表)。不过，Redis 的 hash 做了更多优化。另外，hash 是一个 string 类型的 field 和 value 的映射表，**特别适合用于存储对象**，后续操作的时候，你可以直接仅仅修改这个对象中的某个字段的值。 比如我们可以 hash 数据结构来存储用户信息，商品信息等等。
2. **应用场景:** 系统中对象数据的存储。
3. **命令的使用：**

```bash
127.0.0.1:6379> hset myhash filed1 hello ##设置一个具体的key-value
(integer) 1
127.0.0.1:6379> hget myhash filed1 ##获取一个字段的值
"hello"
127.0.0.1:6379> hmset myhash field1 value1 filed2 value2 #set多个key-value
OK
127.0.0.1:6379> hmget myhash field1 filed2 #获取多个字段值
1) "value1"
2) "value2"
127.0.0.1:6379> hgetall myhash #获取全部的数据
1) "filed1"
2) "hello"
3) "field1"
4) "value1"
5) "filed2"
6) "value2"
127.0.0.1:6379> hdel myhash field1 #删除hash指定的key字段
(integer) 1
127.0.0.1:6379> hgetall myhash
1) "filed1"
2) "hello"
3) "filed2"
4) "value2"

##############################################

# 判断key是否存在和获取hash表的字段数量
127.0.0.1:6379> hlen myhash #获取hash表中字段的数量
(integer) 2
127.0.0.1:6379> hexists myhash filed2 #判断hash指定的字段是否存在
(integer) 1

##############################################
#获取全部的filed 或者value
127.0.0.1:6379> hkeys myhash #获取全部的filed
1) "filed1"
2) "filed2"
127.0.0.1:6379> hvals myhash #获取全部的value
1) "hello"
2) "value2"


#############################################
#incr自增 


127.0.0.1:6379> hincrby myhash cnt 1 #自增+1
(integer) 1
127.0.0.1:6379> hget myhash cnt
"1"
127.0.0.1:6379> hincrby myhash cnt 1
(integer) 2
127.0.0.1:6379> hincrby myhash cnt 3 #指定增量
(integer) 5
127.0.0.1:6379> hget myhash cnt
"5"
127.0.0.1:6379> hincrbyfloat myhash float 1.1 #浮点数的自增
"1.10000000000000009"
127.0.0.1:6379> hincrbyfloat myhash float 1.2
"2.29999999999999982"
127.0.0.1:6379> hget myhash float
"2.29999999999999982"
127.0.0.1:6379> hsetnx myhash filed4 isexits #如果字段不存在则设置
(integer) 1
127.0.0.1:6379> hsetnx myhash filed4 word #如果字段存在  就不能设置
(integer) 0
127.0.0.1:6379> hincrby myhash cnt -2
(integer) 3
```



### 3.5 Zset 类型（有序集合）

1. **介绍：** 和 set 相比，sorted set 增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。
2. **应用场景：** 需要对数据根据某个权重进行排序的场景。比如在直播系统中，实时排行信息包含直播间在线用户列表，各种礼物排行榜，弹幕消息（可以理解为按消息维度的消息排行榜）等信息。
3. **常用命令：**

```bash
127.0.0.1:6379> zadd zset 1 one # 添加元素到zset中， 1 为权重
(integer) 1
127.0.0.1:6379> zadd zset 2 two 3 three #一次加多个元素
(integer) 2
127.0.0.1:6379> zrange zset 0 -1 #查看所有的value
1) "one"
2) "two"
3) "three"

#####################################################
#排序的实现
127.0.0.1:6379> zadd articleViews 100 Java 201 Python 567 C++
(integer) 3
127.0.0.1:6379> zrangebyscore articleViews -inf +inf #按照score从小到大排序
1) "Java"
2) "Python"
3) "C++"
127.0.0.1:6379> zrangebyscore articleViews -inf +inf withscores #从小到大排序且输出score
1) "Java"
2) "100"
3) "Python"
4) "201"
5) "C++"
6) "567"
127.0.0.1:6379> zrevrange articleViews 0 -1 #逆序输出指定区间的元素
1) "C++"
2) "Python"
3) "Java"
127.0.0.1:6379> zrangebyscore articleViews -inf 500 withscores #显示指定score区间的元素
1) "Java"
2) "100"
3) "Python"
4) "201"
127.0.0.1:6379> zrangebyscore articleViews 230 600 withscores
1) "C++"
2) "567"
127.0.0.1:6379> zrange articleViews 0 -1 #获取所有的元素
1) "Java"
2) "Python"
3) "C++"
127.0.0.1:6379> zcard articleViews #获取有序集合中元素的个数
(integer) 3
127.0.0.1:6379> zcount articleViews 1 3 #
(integer) 0
127.0.0.1:6379> zcount articleViews 200 300 #获取指定区间的成员数量
(integer) 1
127.0.0.1:6379> zcount articleViews 200 600
(integer) 2
127.0.0.1:6379> zrem articleViews C++ #移除有序集合中某个元素
(integer) 1
127.0.0.1:6379> zrange articleViews 0 -1
1) "Java"
2) "Python"
```

## 4. 三大特殊类型

### 4.1 geospatial（地理空间）

1. **介绍：** 将指定的地理空间位置（经度、纬度、名称）添加到指定的`key`中。这些数据将会存储到`sorted set`这样的目的是为了方便使用[GEORADIUS](http://www.redis.cn/commands/georadius.html)或者[GEORADIUSBYMEMBER](http://www.redis.cn/commands/georadiusbymember.html)命令对数据进行半径查询等操作。

   该命令以采用标准格式的参数x,y,所以经度必须在纬度之前。这些坐标的限制是可以被编入索引的，区域面积可以很接近极点但是不能索引。具体的限制，由EPSG:900913 / EPSG:3785 / OSGEO:41001 规定如下：

   - 有效的经度从-180度到180度。
   - 有效的纬度从-85.05112878度到85.05112878度。

   当坐标位置超出上述指定范围时，该命令将会返回一个错误。

2. **应用场景：** 朋友的定位，附近的人，打车距离计算等等

3. **常用命令：**geospatial只有6个命令

![image-20210330101916456](https://gitee.com/zhangrenfeng/images/raw/master/img/20210330101924.png)

> GEOADD 
>
> **时间复杂度：**每一个元素添加是O(log(N)) ，N是sorted set的元素数量。

```bash
127.0.0.1:6379> geoadd china:city 113.27324 23.15792 guangzhou #将经度，纬度，空间名称加入到指定的key
(integer) 1
127.0.0.1:6379> geoadd china:city 116.4331 23.25029 shangtou 113.88308 22.55329 shenzhen 121.48941 31.40527 shanghai 116.23128 40.22077 beijing
(integer) 4

#当坐标位置超出  有效的经度从-180度到180度。有效的纬度从-85.05112878度到85.05112878度。 会返回一个错误如下
127.0.0.1:6379> geoadd china:city 181 86 test
(error) ERR invalid longitude,latitude pair 181.000000,86.000000
```

> GEOPOS
>
> **时间复杂度：**O(log(N)) for each member requested, where N is the number of elements in the sorted set.

从key中返回指定位置的经度和纬度，返回的是一个坐标值

```bash
127.0.0.1:6379> geopos china:city shanghai #获取指定城市的经度和纬度
1) 1) "121.48941010236740112"
   2) "31.40526993848380499"
127.0.0.1:6379> geopos china:city shangtou shenzhen #获取多个城市的经度和纬度
1) 1) "116.43309742212295532"
   2) "23.25028874005216295"
2) 1) "113.88307839632034302"
   2) "22.55329111565713873"
```

> GEODIST
>
> **时间复杂度：**O(log(N))

返回两个指定位置之间的距离（直线距离）

如果两个位置中有一个不存在，返回空值

单位：默认是m

- **m** 表示单位为米。
- **km** 表示单位为千米。
- **mi** 表示单位为英里。
- **ft** 表示单位为英尺。

```bash
127.0.0.1:6379> geodist china:city shangtou guangzhou #获取汕头到广州的直线距离
"323185.5581"
127.0.0.1:6379> geodist china:city shangtou guangzhou km
"323.1856"
127.0.0.1:6379> geodist china:city shanghai beijing mi
"676.4539"
```

> GEORADIUS
>
> **时间复杂度：**O(N+log(M))

以给定的经纬度为中心，找出某一半径内的元素

单位：

- **m** 表示单位为米。
- **km** 表示单位为千米。
- **mi** 表示单位为英里。
- **ft** 表示单位为英尺。

可选参数：

- `WITHDIST`: 在返回位置元素的同时， 将位置元素与中心之间的距离也一并返回。 距离的单位和用户给定的范围单位保持一致。
- `WITHCOORD`: 将位置元素的经度和维度也一并返回。
- `WITHHASH`: 以 52 位有符号整数的形式， 返回位置元素经过原始 geohash 编码的有序集合分值。 这个选项主要用于底层应用或者调试， 实际中的作用并不大。
- `ASC`: 根据中心的位置， 按照从近到远的方式返回位置元素。
- `DESC`: 根据中心的位置， 按照从远到近的方式返回位置元素。

```bash
127.0.0.1:6379> georadius china:city 110 20 10000 km #以110 20 的经纬度为中心，寻找10000km内的城市
1) "shenzhen"
2) "guangzhou"
3) "shangtou"
4) "shanghai"
5) "beijing"
127.0.0.1:6379> georadius china:city 110 20 500 km WITHCOORD WITHDIST #withdist 显示到中间的距离
1) 1) "shenzhen"
   2) "492.5245"
   3) 1) "113.88307839632034302"
      2) "22.55329111565713873"
2) 1) "guangzhou"
   2) "487.7948"
   3) 1) "113.27324062585830688"
      2) "23.1579209662846921"
127.0.0.1:6379> georadius china:city 110 20 500 km WITHCOORD WITHDIST WITHHASH count 1 #返回指定城市的数量
1) 1) "guangzhou"
   2) "487.7948"
   3) (integer) 4046534010880445
   4) 1) "113.27324062585830688"
      2) "23.1579209662846921"
```

> GEORADIUSBYMEMBER
>
> **时间复杂度：**O(N+log(M))

和GEORADIUS命令类似，但不同的是中心点的位置是由位置元素决定的，而不是输入经度纬度。

```bash
127.0.0.1:6379> georadiusbymember china:city shangtou 1000 km #以指定元素位置找出周围其他元素
1) "shenzhen"
2) "guangzhou"
3) "shangtou"
```

> GEOHASH
>
> **时间复杂度：**O(log(N)) 

该命令将返回11个字符的Geohash字符串，所以没有精度Geohash，损失相比，使用内部52位表示。返回的geohashes具有以下特性：

1. 他们可以缩短从右边的字符。它将失去精度，但仍将指向同一地区。
2. 与类似的前缀字符串是附近，但相反的是不正确的，这是可能的，用不同的前缀字符串附近。

```bash
127.0.0.1:6379> geohash china:city beijing shangtou #将二维的经纬度转化为一维的字符串，如果两个字符串越接近，那么距离则越近
1) "wx4sucvncn0"
2) "ws4u32xfxz0"
127.0.0.1:6379> geohash china:city shenzhen guangzhou
1) "ws0br3xnkn0"
2) "ws0e9xg09v0"
```

### 4.2 HyperLogLog结构



1. **介绍：** Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定 的、并且是很小的。

   在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基 数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。

   但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。

   优点：占用的内存是固定的

2. **应用场景：** 网站UV的统计

3. **常用命令：**

```bash
127.0.0.1:6379> PFadd siteuv a b c d e f g #创建第一组元素
(integer) 1
127.0.0.1:6379> PFcount siteuv #查看第一组元素的个数
(integer) 7
127.0.0.1:6379> PFadd siteuv2 h i j k a b c
(integer) 1
127.0.0.1:6379> PFcount siteuv2
(integer) 7
127.0.0.1:6379> PFmerge siteuv3 siteuv siteuv2 #合并两组不重复的元素
OK
127.0.0.1:6379> PFcount siteuv3
(integer) 11
```

### 4.3 bitmap

1. **介绍 ：** bitmap 存储的是连续的二进制数字（0 和 1），通过 bitmap, 只需要一个 bit 位来表示某个元素对应的值或者状态，key 就是对应元素本身 。我们知道 8 个 bit 可以组成一个 byte，所以 bitmap 本身会极大的节省储存空间。
2. **应用场景:** 适合需要保存状态信息（比如是否签到、是否登录...）并需要进一步对这些信息进行分析的场景。比如用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。

```bash
127.0.0.1:6379> setbit daka 1 1 #记录打开天数  1 代表有打卡 0 代表没打卡
(integer) 0
127.0.0.1:6379> setbit daka 2 0
(integer) 0
127.0.0.1:6379> setbit daka 3 0
(integer) 0
127.0.0.1:6379> setbit daka 4 1
(integer) 0
127.0.0.1:6379> setbit daka 5 0
(integer) 0
127.0.0.1:6379> setbit daka 6 0
(integer) 0
127.0.0.1:6379> setbit daka 7 1
(integer) 0
127.0.0.1:6379> getbit daka 7 #获取对应的key指定offset的value
(integer) 1
127.0.0.1:6379> getbit daka 8
(integer) 0
127.0.0.1:6379> bitcount daka #统计value为1的offset数量
(integer) 3
```

## 5. Redis事务

Redis 可以通过 **MULTI，EXEC，DISCARD 和 WATCH** 等命令来实现事务(transaction)功能。

事务可以一次执行多个命令， 并且带有以下两个重要的保证：

- 事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。
- 事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。

`EXEC`命令负责触发并执行事务中的所有命令：

- 如果客户端在使用 `MULTI` 开启了一个事务之后，却因为断线而没有成功执行 `EXEC`，那么事务中的所有命令都不会被执行。
- 另一方面，如果客户端成功在开启事务之后执行 `EXEC` ，那么事务中的所有命令都会被执行。

事务中的错误：

* 事务在执行EXEC之前，入队的命令出错，比如命令有语法错误，内存不足等等，就会入队失败。如果有命令在入队时失败，Redis会取消这个事务
* 事务在EXEC调用后失败。比如事务中的命令处理错误类型的键（自增一个字符串），即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。

事务具有ACID四大特性：原子性，一致性，隔离性，持久性。

但Redis的事务跟上面所说的关系型数据库的事务不一样。Redis不支持roll back 不满足原子性和持久性。

**为什么不支持回滚（roll back）？**

- Redis 命令只会因为错误的语法而失败（并且这些问题不能在入队时发现），或是命令用在了错误类型的键上面：这也就是说，从实用性的角度来说，失败的命令是由编程错误造成的，而这些错误应该在开发的过程中被发现，而不应该出现在生产环境中。
- 因为不需要对回滚进行支持，所以 Redis 的内部可以保持简单且快速。

```bash
127.0.0.1:6379> multi #开启一个事务
OK
#命令入队列
127.0.0.1:6379> set k1 v1
QUEUED
127.0.0.1:6379> set k2 v3
QUEUED
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> exec #执行事务
1) OK
2) OK
3) "v3"
```

事务在执行EXEC之前出错  Redis会取消这个事务

```bash
127.0.0.1:6379> multi
OK
127.0.0.1:6379> set 1 1
QUEUED
127.0.0.1:6379> sett 2 2
(error) ERR unknown command `sett`, with args beginning with: `2`, `2`,
127.0.0.1:6379> exec
(error) EXECABORT Transaction discarded because of previous errors.
```

事务在调用EXEC后出错  即使事务中有某个/某些命令在执行时产生了错误， 事务中的其他命令仍然会继续执行。

```bash
127.0.0.1:6379> set k1 v1
OK
127.0.0.1:6379> multi
OK
127.0.0.1:6379> incr k1 #v1不能自增
QUEUED
127.0.0.1:6379> set k2 v2
QUEUED
127.0.0.1:6379> get k2
QUEUED
127.0.0.1:6379> get k1
QUEUED
127.0.0.1:6379> exec
1) (error) ERR value is not an integer or out of range #这条命令报错  其他依旧执行
2) OK
3) "v2"
4) "v1"
127.0.0.1:6379> get k2 #可以获取到事务中设置的值
"v2"
```

放弃事务：DISCARD

```bash
127.0.0.1:6379> multi #开启一个事务
OK
127.0.0.1:6379> set aaa a
QUEUED
127.0.0.1:6379> discard #放弃事务
OK
127.0.0.1:6379> get aaa
(nil)
```



## 6. Redis乐观锁

**悲观锁：**

执行加锁操作时很"悲观"，认为什么时候都会出现问题，无论做什么都会加锁

**乐观锁：**

乐观锁是相对悲观锁而言的，乐观锁假设数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则返回给用户错误的信息，让用户决定如何去做。

在Redis中，可以使用watch命令去为事务实现乐观锁，原理是WATCH 命令可以为 Redis 事务提供 check-and-set （CAS）行为。

被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC执行之前被修改了， 那么整个事务都会被取消， EXEC返回nil-reply来表示事务已经失败。

举个例子：我们同时打开两个redis的客户端

线程1：

```bash
127.0.0.1:6379> set money 100 #设置钱一共100
OK
127.0.0.1:6379> set out 0 #设置支出为0
OK
127.0.0.1:6379> watch money out #监视money和out键
OK
127.0.0.1:6379> multi
OK
127.0.0.1:6379> decrby money 10 #钱减少10
QUEUED
127.0.0.1:6379> incrby out 10 #支出增加10
QUEUED
127.0.0.1:6379> exec #在执行exec时，线程2执行增加钱100的命令  在线程2结束后，在执行exec，发现事务失败，返回nil
(nil)
127.0.0.1:6379> get money #确认事务没有完成
"200"
127.0.0.1:6379> get out
"0"
```

线程2：

```bash
127.0.0.1:6379> incrby money 100 #给money增加100
(integer) 200
```

## 7. Redis配置文件

> 单位配置 units 对大小写不敏感

![image-20210401192138346](https://gitee.com/zhangrenfeng/images/raw/master/img/20210401192159.png)

> 包含（include）  跟c++的include类似  导入的意思

![image-20210401192339941](https://gitee.com/zhangrenfeng/images/raw/master/img/20210401192340.png)

> 网络（network） 

````bash
bind 127.0.0.1 #默认绑定的ip
protected-mode yes #开启保护模式
port 6379 #默认的端口号
````

> 通用（GENERAL）

```bash
daemonize yes #以守护进程的方式运行，默认是no  需要自己开启

pidfile /var/run/redis_6379.pid #如果以后台的方式运行，需要指定一个pid文件
#开启日志  日志级别
# debug (a lot of information, useful for development/testing)
# verbose (many rarely useful info, but not a mess like the debug level)
# notice (moderately verbose, what you want in production probably)
# warning (only very important / critical messages are logged)
loglevel notice   #开启日志 级别为notice
logfile "" #日志文件位置名
databases 16 #默认有16个数据库
always-show-logo yes #是否总是显示logo
```

> 快照（SNAPSHOTTING）

Redis是内存数据库，如果没有持久化，那么数据断电即失。

持久化，在规定的时间内，执行了多少次操作，则会持久化到 .rdb文件或 .aof文件

```bash
save 900 1 #在900s内，如果至少有1个key被修改，那么会进行持久化操作
save 300 10 #在300s内，如果至少有10个key被修改，那么会进行持久化操作
save 60 10000 #在60s内，如果至少有10000个key被修改，那么会进行持久化操作

stop-writes-on-bgsave-error yes #如果持久化失败 redis是否还继续工作
rdbcompression yes #是否压缩rdb文件
rdbchecksum yes #保存rdb文件时  是否进行检查
dir ./ #rdb文件保存的位置
```

> 安全（SECURITY）

这里可以设置redis的密码，默认是没有密码的



![image-20210401195256880](https://gitee.com/zhangrenfeng/images/raw/master/img/20210401195257.png)

```bash
127.0.0.1:6379>  ping
PONG
127.0.0.1:6379> config get requirepass #获取redis密码
1) "requirepass"
2) ""
127.0.0.1:6379> config set requirepass 123456 #设置redis密码
OK
127.0.0.1:6379> ping  #提示没有权限
(error) NOAUTH Authentication required.
127.0.0.1:6379> set name name
(error) NOAUTH Authentication required.
127.0.0.1:6379> auth 123456 #使用密码登录
OK
127.0.0.1:6379> ping
PONG
127.0.0.1:6379> config get requirepass
1) "requirep
```



> 限制（LIMITS）

```bash
maxclients 10000 #能连接上redis的最大客户端数量
maxmemory <bytes> #redis配置最大容量
maxmemory-policy noeviction #内存达到上限之后的处理策略
1、volatile-lru：只对设置了过期时间的key进行LRU（默认值） 
2、allkeys-lru ： 删除lru算法的key   
3、volatile-random：随机删除即将过期key   
4、allkeys-random：随机删除   
5、volatile-ttl ： 删除即将过期的   
6、noeviction ： 永不过期，返回错误
```

>  APPEND ONLY 模式  aof配置

```bash
appendonly no #默认不开启aof，rdb完全够用
appendfilename "appendonly.aof" #持久化的文件名称

# appendfsync always  #每次修改都会sync，会消耗性能
appendfsync everysec #每秒执行一次sync，可能会丢失这1s的数据
# appendfsync no #不执行sync，操作系统自己同步数据，速度快
```



## 8. Redis持久化

持久化数据是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。



Redis支持持久化，有两种不同的持久化操作。一种持久化方式叫做**快照（RDB）**，另外一种是**只追加文件（AOF）**。



### 8.1 快照持久化（RDB）

Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。



快照持久化是Redis默认采用的持久化方式，在Redis.conf配置文件可以配置。

**触发机制：**

* save 快照持久化

![image-20210402190656518](https://gitee.com/zhangrenfeng/images/raw/master/img/20210402190705.png)

使用save命令，会立刻对当前内存中的数据进行持久化，但是会阻塞，在此期间不接受其他操作

缺点：由于save命令是同步的，会占用Redis的主进程。若Redis数据非常多时，save命令执行速度会变慢，阻塞所有客户端请求。

* bgsave 快照持久化

![image-20210402190945934](https://gitee.com/zhangrenfeng/images/raw/master/img/20210402190947.png)

Redis会单独创建一个子进程来持久化，会先将数据写入到一个临时文件，待持久化过程结束了，再用这个临时文件替代上次持久化好的文件。

优点：主进程不会进行任何IO操作，确保了极高的性能。

缺点：对数据恢复到完整性不敏感



save和bgsave对比

![image-20210402191027732](https://gitee.com/zhangrenfeng/images/raw/master/img/20210402191028.png)



**自动触发：**

通过配置 `save m n`的方式，表示在m秒内数据集存在n次修改时，系统自动触发bgsave操作。

```bash
save 900 1           #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发BGSAVE命令创建快照。

save 300 10          #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发BGSAVE命令创建快照。

save 60 10000        #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发BGSAVE命令创建快照。
```

#### 总结：

**优点：**

* RDB文件小，非常适合定时备份，用于灾难恢复
* 因为RDB文件中直接存储的是内存数据，而AOF文件中存储的是一条条的命令，需要应用命令。Redis加载RDB文件速度比AOF快

**缺点：**

* RDB持久化方式不能做到实时/秒级持久化
* 实时持久化要全量刷内存到磁盘，成本太高
* 每秒fork子进程也会阻塞主进程，影响性能

### 8.2 追加文件持久化（AOF)

AOF对RDB的缺点进行了优化，在使用AOF持久化方式时，Redis会将每个收到的写操作命令都通过Write函数追加到文件的最后。当Redis重启时会通过执行文件中保存的写命令来在内存中重建整个数据库的内容。

**什么是AOF重写？**

因为AOF的运作方式是不断将命令追加到文件的末尾，所以这样就会导致AOF文件越来越大。

当其中如果对一个计数器进行了100次的`incr key`，就需要保存100条命令。这样就显得十分多余，因为我们可以通过`set key value`命令来代替。

所以为了处理这种情况，Redis提出了AOF重写，即在不打断客户端的情况下，对AOF文件进行重建。

执行`bgrewriteaof`命令，Redis将生成一个新的AOF文件，这个文件包含当前数据集所需的最少命令。

在Redis2.4后会自动触发AOF重写。



**AOF持久化过程：**

![image-20210402192553799](https://gitee.com/zhangrenfeng/images/raw/master/img/20210402192555.png)

> 1. 客户端发布bgrewriteaof命令
> 2. redis主进程fork子进程
> 3. 父进程进行处理client的请求，除了把写命令写入到原来的aof文件，同时把收到的写命令缓存到**AOF重写缓冲区**，这样就能保证如果子进程重写失败的话不会出现问题
> 4. 子进程根据内存快照，按照命令合并规则写入到新AOF文件
> 5. 当子进程把内存快照写入到临时文件中后，子进程发信号通知父进程把缓存的写命令也写入到临时文件
> 6. 将临时文件替换老的aof文件，并重命名，后面收到的写命令也开始往新的aof文件中追加



**触发机制：**

1. 手动触发：执行bgrewriteaof命令
2. 根据配置触发：编写配置文件

```bash
appendonly yes
```



Redis可以使用 Redis 附带的 `redis-check-aof` 程序，对原来的 AOF 文件进行修复

```bash
redis-check-aof --fix
```



#### 总结:

**优点：**

* 使用 AOF 持久化会让 Redis 变得非常耐久。文件完整性更强
* AOF有重写机制
* AOF 文件有序地保存了对数据库执行的所有写入操作

```bash
appendfsync always    #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度
appendfsync everysec  #每秒钟同步一次，显示地将多个写命令同步到硬盘
appendfsync no        #让操作系统决定何时进行同步
```

为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。

**缺点：**

* 相对于数据文件来说， AOF 远远大于 RDB ，修复的速度也慢很多

* AOF 都是 IO 操作，所以效率也比 RDB 低



**RDB和AOF，选哪个一个？**

Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 `aof-use-rdb-preamble` 开启）。

如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。

## 9. Redis订阅发布

Redis 发布订阅（pub/sub）是一种消息通信模式：发送者（pub）发送消息，订阅者（sub）接收消息。

Redis客户端可以订阅任意数量的频道。

Redis的`subscribe`命令可以让客户端订阅任意数量的频道，每当有新消息发送到被订阅的频道时，信息就会被发送给所有订阅指定频道的客户端。

![image-20210403133402296](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403133410.png)



当有新消息通过`publish`命令发送给频道时，这个消息就会被发送给订阅它的三个客户端：

![image-20210403133601228](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403133602.png)

> 订阅频道

每个Redis服务器进程都维持着一个表示服务器状态的`redis.h/redisServer`结构，结构的pubsub_channels属性是一个字典，这个字典就用于保存订阅频道的信息：

```c++
struct redisServer{
    dict *pubsub_channels;
}
```

其中，字典的键为正在被订阅的频道，而字典的值是一个链表，链表中保存了所有订阅这个频道的客户端。

![image-20210403134338253](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403134339.png)

当客户端调用`subscribe`命令时，程序就将客户端加到要订阅的频道的链表上（使用尾插法）。

![image-20210403134455344](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403134456.png)

通过这个`pubsub_channels`字典检查某个频道是否为字典的键，就能知道该频道是否被订阅，只要取出某个键的值，就获取到订阅该频道的所有客户端信息。

> 发送信息到频道

当调用`publish channel message`命令时，程序首先根据channel定位到字典的键，然后将信息发送给字典值链表中的所有客户端。

![image-20210403134850751](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403134852.png)

> 退订频道

使用`unsubscribe`命令推定指定频道，工作原理：从`pubsub_channels`字典给定频道中，删除关于当前客户端的信息（链表的删除）。



**实例：**

第一个redis客户端创建一个频道为wechat

```bash
127.0.0.1:6379> subscribe wechat
Reading messages... (press Ctrl-C to quit)
1) "subscribe"
2) "wechat"
3) (integer) 1
```

第二个客户端发送信息到这个频道

```bash
127.0.0.1:6379> publish wechat hello
(integer) 1
```

第三个客户端发送信息到这个频道

```bash
127.0.0.1:6379> publish wechat hi
(integer) 1
127.0.0.1:6379> publish wechat dadadada
(integer) 1
```

此时第一个客户端创建的频道就可以看到其他客户端发送的信息

```bash
127.0.0.1:6379> subscribe wechat
Reading messages... (press Ctrl-C to quit)
1) "subscribe"
2) "wechat"
3) (integer) 1
1) "message"
2) "wechat"
3) "hello"
1) "message"
2) "wechat"
3) "hi"
1) "message"
2) "wechat"
3) "datealive"
1) "message"
2) "wechat"
3) "dadadada"
```



> 模式的订阅与信息发送

当使用`publish`命令发送信息到某个频道时，不仅所有订阅该频道的客户端会收到信息，如果有某个/某些模式和这个频道匹配的话，那么所有订阅这个/这些频道的客户端也会收到信息。

![image-20210403135601265](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403135602.png)



> 订阅模式

订阅模式的 `redisServer.pubsub_patterns`属性是一个链表，链表中保存着所有和模式相关的信息。

```c++
struct redisServer{
    list *pubsub_patterns;
}
```

链表的每个节点都包含一个`redis.h/pubsubPattern`结构

```c++
typedef struct pubsubPattern{
    redisClient *client; //保存订阅模式的客户端
    robj *pattern;//保存被订阅的模式
}pubsubPattern;
```

每当调用 `PSUBSCRIBE` 命令订阅一个模式时， 程序就创建一个包含客户端信息和被订阅模式的 `pubsubPattern` 结构， 并将该结构添加到 `redisServer.pubsub_patterns` 链表中。

![image-20210403140249537](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403140250.png)



> 发送信息到模式

模式的发送信息，除了将信息发送到订阅频道的客户端之外，还会将channel和pubsub_patterns中的模式进行对比，如果channel和某个模式匹配的话，也将信息发送到订阅这个模式的客户端。

![image-20210403140702338](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403140703.png)

> 退订模式

使用`punsubscribe`命令可以退订指定的模式， 这个命令执行的是订阅模式的反操作： 程序会删除 `redisServer.pubsub_patterns` 链表中， 所有和被退订模式相关联的 `pubsubPattern` 结构， 这样客户端就不会再收到和模式相匹配的频道发来的信息。

![image-20210403153521952](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403153522.png)



> 总结

- 订阅信息由服务器进程维持的 `redisServer.pubsub_channels` 字典保存，字典的键为被订阅的频道，字典的值为订阅频道的所有客户端。
- 当有新消息发送到频道时，程序遍历频道（键）所对应的（值）所有客户端，然后将消息发送到所有订阅频道的客户端上。
- 订阅模式的信息由服务器进程维持的 `redisServer.pubsub_patterns` 链表保存，链表的每个节点都保存着一个 `pubsubPattern` 结构，结构中保存着被订阅的模式，以及订阅该模式的客户端。程序通过遍历链表来查找某个频道是否和某个模式匹配。
- 当有新消息发送到频道时，除了订阅频道的客户端会收到消息之外，所有订阅了匹配频道的模式的客户端，也同样会收到消息。
- 退订频道和退订模式分别是订阅频道和订阅模式的反操作。



## 10. Redi主从复制

> 主从复制的概念

主从复制，是指将一台Redis服务器的数据，复制到其他的Redis服务器。前者称为主节点(master)，后者称为从节点(slave)；数据的复制是单向的，只能由主节点到从节点。

默认情况下，每台Redis服务器都是主节点；且一个主节点可以有多个从节点(或没有从节点)，但一个从节点只能有一个主节点。

**主从复制的作用**

主从复制的作用主要包括：

1. 数据冗余：主从复制实现了数据的热备份，是持久化之外的一种数据冗余方式。
2. 故障恢复：当主节点出现问题时，可以由从节点提供服务，实现快速的故障恢复；实际上是一种服务的冗余。
3. 负载均衡：在主从复制的基础上，配合读写分离，可以由主节点提供写服务，由从节点提供读服务（即写Redis数据时应用连接主节点，读Redis数据时应用连接从节点），分担服务器负载；尤其是在写少读多的场景下，通过多个从节点分担读负载，可以大大提高Redis服务器的并发量。
4. 高可用基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是Redis高可用的基础。

> 如何使用主从复制

#### 1.建立复制

**主从复制的开启，是在从节点发起的，不需要在主节点做任何事情。**

从节点开启主从复制，有3种方式

* 配置文件 

在从服务器的配置文件中加入slaveof  <masterip> <masterport>

* 启动命令

redis-server启动命令后加入 --slaveof <masterip> <masterport>

* 客户端命令

Redis服务器启动后，直接通过客户端执行命令：slaveof <masterip> <masterport>，则该Redis实例成为从节点。

#### 2.实例

以win10系统下为例：

**准备工作：开启三个节点**

> 1.配置文件
> 将redis.windows-service.conf复制一份，改名为相应文件，并更改配置文件中的端口为指定端口，以6380为例
>
> port 6380
>
> 2.安装服务
> redis-server --service-install --service-name redis_6380 redis.windows-service-6380.conf
> 3.启动服务
> redis-server --service-start --service-name redis_6380
> 4.停止服务
> redis-server --service-stop --service-name redis_6380
> 5.卸载服务
> redis-server --service-uninstall --service-name redis_6380

![image-20210403220644535](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403220645.png)



![image-20210403220712708](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403220713.png)



![image-20210403220722720](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403220723.png)

**建立复制：**

客户端执行`slaveof`命令，把80和81设置为79的从机

![image-20210403221233586](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403221235.png)



**效果：**

![image-20210403221607764](https://gitee.com/zhangrenfeng/images/raw/master/img/20210403221608.png)

> 主从复制的原理

Redis主从复制可以根据是否是全量分为全量同步和增量同步。

**全量同步：**

Redis 全量复制一般发生在Slave初始化阶段，这时Slave需要将Master上的所有数据都复制一份。步骤如下：

* 从服务器连接主服务器，发送SYNC命令
* 主服务器接受到SYNC命令，开始执行BGSAVE命令生成RDB文件并使用缓冲区记录此后的所以写命令
* 主服务区BGSAVE执行完后，向所有从服务器发送快照文件，并在发送期间继续记录被执行的写命令
* 从服务器收到快照后丢弃所有旧数据，载入收到的快照
* 主服务器快照发送完毕后开始向从服务器发送缓冲区中的写命令
* 从服务器完成对快照的载入，开始接受命令请求，并执行来自主服务器缓冲区的命令

![image-20210405103201720](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405103210.png)

**增量同步：**

Redis增量复制是指Slave初始化后开始正常工作时  主服务器发生的写操作同步到 从服务器的过程。

增量复制的过程主要是主服务器每执行一个写命令就会向从服务器发生相同的写命令，从服务器接受并执行收到的写命令。

**Redis主从同步策略：**
主从刚刚连接的时候，进行全量同步；全同步结束后，进行增量同步。当然，如果有需要，slave 在任何时候都可以发起全量同步。redis 策略是，无论如何，首先会尝试进行增量同步，如不成功，要求从机进行全量同步。

## 11. 哨兵模式

> **主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。**这不是一种推荐的方式，更多时候，我们优先考虑**哨兵模式**。

哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是**哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。**

![image-20210405125647383](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405125648.png)

这里的哨兵有两个作用

* 通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器
* 当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让他们切换主机。

然后一个哨兵进程对Redis服务进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。

假设主服务器宕机，哨兵1先检测到这个结果，系统并不会马上进行**failover（故障切换）**过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为**主观下线**。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为**客观下线**。这样对于客户端而言，一切都是透明的。

> 启动哨兵（Sentinel）

对于 redis-sentinel 程序， 你可以用以下命令来启动 Sentinel 系统：

对于 redis-server 程序， 你可以用以下命令来启动一个运行在 Sentinel 模式下的 Redis 服务器：

```bash
redis-server /path/to/sentinel.conf --sentinel
```

两种方法都可以启动一个 Sentinel 实例。

启动 Sentinel 实例必须指定相应的配置文件， 系统会使用配置文件来保存 Sentinel 的当前状态， 并在 Sentinel 重启时通过载入配置文件来进行状态还原。

如果启动 Sentinel 时没有指定相应的配置文件， 或者指定的配置文件不可写（not writable）， 那么 Sentinel 会拒绝启动。

> 配置哨兵

创建编写 sentinel.conf 文件

完整配置文件内容解读如下： 

```bash
# Example sentinel.conf
 
# 哨兵sentinel实例运行的端口 默认26379
port 26379
 
# 哨兵sentinel的工作目录
dir /tmp
 
# 哨兵sentinel监控的redis主节点的 ip port 
# master-name  可以自己命名的主节点名字 只能由字母A-z、数字0-9 、这三个字符".-_"组成。
# quorum 当这些quorum个数sentinel哨兵认为master主节点失联 那么这时 客观上认为主节点失联了
# sentinel monitor <master-name> <ip> <redis-port> <quorum>
sentinel monitor mymaster 127.0.0.1 6379 1
 
# 当在Redis实例中开启了requirepass foobared 授权密码 这样所有连接Redis实例的客户端都要提供密码
# 设置哨兵sentinel 连接主从的密码 注意必须为主从设置一样的验证密码
# sentinel auth-pass <master-name> <password>
sentinel auth-pass mymaster MySUPER--secret-0123passw0rd
 
 
# 指定多少毫秒之后 主节点没有应答哨兵sentinel 此时 哨兵主观上认为主节点下线 默认30秒
# sentinel down-after-milliseconds <master-name> <milliseconds>
sentinel down-after-milliseconds mymaster 30000
 
# 这个配置项指定了在发生failover主备切换时最多可以有多少个slave同时对新的master进行 同步，
# 这个数字越小，完成failover所需的时间就越长，
# 但是如果这个数字越大，就意味着越 多的slave因为replication而不可用。
# 可以通过将这个值设为 1 来保证每次只有一个slave 处于不能处理命令请求的状态。
# sentinel parallel-syncs <master-name> <numslaves>
sentinel parallel-syncs mymaster 1
 
 
 
# 故障转移的超时时间 failover-timeout 可以用在以下这些方面： 
#1. 同一个sentinel对同一个master两次failover之间的间隔时间。
#2. 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。
#3.当想要取消一个正在进行的failover所需要的时间。  
#4.当进行failover时，配置所有slaves指向新的master所需的最大时间。不过，即使过了这个超时，slaves依然会被正确配置为指向master，但是就不按parallel-syncs所配置的规则来了
# 默认三分钟
# sentinel failover-timeout <master-name> <milliseconds>
sentinel failover-timeout mymaster 180000
 
# SCRIPTS EXECUTION
 
#配置当某一事件发生时所需要执行的脚本，可以通过脚本来通知管理员，例如当系统运行不正常时发邮件通知相关人员。
#对于脚本的运行结果有以下规则：
#若脚本执行后返回1，那么该脚本稍后将会被再次执行，重复次数目前默认为10
#若脚本执行后返回2，或者比2更高的一个返回值，脚本将不会重复执行。
#如果脚本在执行过程中由于收到系统中断信号被终止了，则同返回值为1时的行为相同。
#一个脚本的最大执行时间为60s，如果超过这个时间，脚本将会被一个SIGKILL信号终止，之后重新执行。
 
#通知型脚本:当sentinel有任何警告级别的事件发生时（比如说redis实例的主观失效和客观失效等等），将会去调用这个脚本，
#这时这个脚本应该通过邮件，SMS等方式去通知系统管理员关于系统不正常运行的信息。调用该脚本时，将传给脚本两个参数，
#一个是事件的类型，
#一个是事件的描述。
#如果sentinel.conf配置文件中配置了这个脚本路径，那么必须保证这个脚本存在于这个路径，并且是可执行的，否则sentinel无法正常启动成功。
#通知脚本
# sentinel notification-script <master-name> <script-path>
  sentinel notification-script mymaster /var/redis/notify.sh
 
# 客户端重新配置主节点参数脚本
# 当一个master由于failover而发生改变时，这个脚本将会被调用，通知相关的客户端关于master地址已经发生改变的信息。
# 以下参数将会在调用脚本时传给脚本:
# <master-name> <role> <state> <from-ip> <from-port> <to-ip> <to-port>
# 目前<state>总是“failover”,
# <role>是“leader”或者“observer”中的一个。 
# 参数 from-ip, from-port, to-ip, to-port是用来和旧的master和新的master(即旧的slave)通信的
# 这个脚本应该是通用的，能被多次调用，不是针对性的。
# sentinel client-reconfig-script <master-name> <script-path>
sentinel client-reconfig-script mymaster /var/redis/reconfig.sh
```

**实例：（只配置一个哨兵）**

配置sentinel.conf文件

```bash
port 26379
sentinel monitor mymaster 127.0.0.1 6379 1
sentinel down-after-milliseconds mymaster 3000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 10000
```

运行哨兵

![image-20210405132912925](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405132914.png)



**测试：**

将主服务器6379宕机

查看效果：

![image-20210405133506614](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405133508.png)



![image-20210405133525318](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405133526.png)

![image-20210405133624855](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405133625.png)



如主机此时回来了，也只能归并到新的从机下，当作从机。



**总结：**

优点：
1、哨兵集群，基于主从复制模式，所有的主从配置优点，它都有
2、主从可以切换，故障可以转移，高可用性的系统
3、哨兵模式就是主从模式的升级，手动到自动，更加健壮
缺点：
1、Redis不好在线扩容的，集群容量一旦到达上限，在线扩容就十分麻烦
2、哨兵模式的配置繁琐

## 12. Redis缓存穿透

> 什么是缓存穿透？

缓存穿透就是大量请求的key不存在缓存中，导致请求直接到了数据库上，根本没有经过缓存。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。

> 解决方法？

最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。



* **缓存无效key**

如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间，具体命令如下： `SET key value EX 10086` 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。

* **布隆过滤器**

布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。

具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。

![image-20210405170701815](https://gitee.com/zhangrenfeng/images/raw/master/img/20210405170702.png)

但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： **布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。**

*为什么会出现误判的情况呢? 我们还要从布隆过滤器的原理来说！*

我们先来看一下，**当一个元素加入布隆过滤器中的时候，会进行哪些操作：**

1. 使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。
2. 根据得到的哈希值，在位数组中把对应下标的值置为 1。

我们再来看一下，**当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行哪些操作：**

1. 对给定元素再次进行相同的哈希计算；
2. 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。

然后，一定会出现这样一种情况：**不同的字符串可能哈希出来的位置相同。** （可以适当增加位数组大小或者调整我们的哈希函数来降低概率）

* **存在的问题：**

如果空值能够被缓存起来，这就意味着缓存需要更多的空间存储更多的键，因为这当中可能会有很多的空值的键。

即使对空值设置了过期时间，还是会存在 缓存层和存储层 的数据会有一段时间窗口的不一致的问题，对于需要保持一致性的业务会有影响。

## 13. 缓存击穿

> 什么是缓存击穿？

相较于缓存穿透，缓存击穿的目的性更强，一个存在的key，在缓存过期的一刻，同时有大量的请求，这些请求都会击穿到DB，造成瞬时DB请求量大、压力骤增。这就是缓存被击穿，只是针对其中某个key的缓存不可用而导致击穿，但是其他的key依然可以使用缓存响应。

 比如热搜排行上，一个热点新闻被同时大量访问就可能导致缓存击穿。

> 解决方法？

* 设置热点数据永不过期

这样就不会出现热点数据过期的情况，但是当Redis内存空间满的时候也会清理部分数据，而且此种方案会占用空间，一旦热点数据多了起来，就会占用部分空间。

* 加互斥锁(分布式锁)

在访问key之前，采用SETNX（set if not exists）来设置另一个短期key来锁住当前key的访问，访问结束再删除该短期key。保证同时刻只有一个线程访问。这样对锁的要求就十分高。



## 14. 缓存雪崩

> 什么是缓存雪崩？

大量的key设置了相同的过期时间 或者 Redis 服务器宕机，导致在缓存在同一时刻全部失效，造成瞬时DB请求量大、压力骤增，引起雪崩。

举个例子 ：秒杀开始 12 个小时之前，我们统一存放了一批商品到 Redis 中，设置的缓存过期时间也是 12 个小时，那么秒杀开始的时候，这些秒杀的商品的访问直接就失效了。导致的情况就是，相应的请求直接就落到了数据库上，就像雪崩一样可怕。

> 解决方法？

* redis高可用

这个思想的含义是，既然redis有可能挂掉，那我多增设几台redis，这样一台挂掉之后其他的还可以继续工作，其实就是搭建的集群

* 限流降级

这个解决方案的思想是，在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。

* 数据预热

数据加热的含义就是在正式部署之前，我先把可能的数据先预先访问一遍，这样部分可能大量访问的数据就会加载到缓存中。在即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。


## 15 .分布式缓存技术Redis和Memcached的区别和共同点

> 分布式缓存主要解决的是单机缓存的容量受服务器限制并且无法保存通用的信息。因为，本地缓存只在当前服务里有效，比如如果你部署了两个相同的服务，他们两者之间的缓存数据是无法共同的。

**共同点：**

* 都是基于内存的数据库，一般都用来当做缓存使用
* 都有过期策略
* 性能非常高

**区别:**

1. **Redis 支持更丰富的数据类型（支持更复杂的应用场景）**。Redis 不仅仅支持简单的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。
2. **Redis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memecache 把数据全部存在内存之中。**
3. **Redis 有灾难恢复机制。** 因为可以把缓存中的数据持久化到磁盘上。
4. **Redis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。但是，Memcached 在服务器内存使用完之后，就会直接报异常。**
5. **Memcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 Redis 目前是原生支持 cluster 模式的.**
6. **Memcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的多路 IO 复用模型。** （Redis 6.0 引入了多线程 IO ）
7. **Redis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。并且，Redis 支持更多的编程语言。**
8. **Memcached 过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。**



面试高频题待续。。。。。

参考：

- 《Redis 开发与运维》
- 《Redis 设计与实现》
- 《javaGuide》
- b站狂神说Redis视频



